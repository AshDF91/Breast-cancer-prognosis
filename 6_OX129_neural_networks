## Neural network (competing risks) modelling ##

## Same overall process as XGBoost - use stacked imputed datasets, fit model to entire data, 
## evaluate using internal-external cross-validation that recapitulates hyperparameter tuning (nested)


#############################
## Load in packages needed ##
#############################
library(survival)
library(readr)
library(stringr)
library(caret)
library(haven)
library(ParBayesianOptimization) 
library(keras)
library(tensorflow)

########################################################
## Check and set working directory for importing data ##
########################################################
getwd() 
setwd("/final_datasets/ML/")

##################
## Load in data ##
##################
data <- read_dta("OX129_endpoint3_stacked50_pseudovalues.dta")
str(data)

#########################
## Variable formatting ##
#########################
## Categorical variables need to be converted into dummy variables ##
## First, reformat relevant parameters as factors, then convert to dummies ##

data$smoke_cat <- factor(data$smoke_cat)                         # Smoking status 
data$cancer_route <- factor(data$cancer_route)                   # Route to diagnosis
data$pr_status2 <- factor(data$pr_status2)                       # PR +/-
data$her2_status2 <- factor(data$her2_status2)                   # HER2 +/-
data$er_status2 <- factor(data$er_status2)                       # ER +/- 
data$radiotherapy <- factor(data$radiotherapy)                   # Use of RTx in 1st year 
data$mastectomy <- factor(data$mastectomy)                       # Mastectomy in 1st year  
data$other_surgery <- factor(data$other_surgery)                 # Other surgery in 1st year 
data$cancer_stage <- factor(data$cancer_stage)                   # Stage (I-IV)
data$cancer_grade <- factor(data$cancer_grade)                   # Grade (differentiation)
data$hrt <- factor(data$hrt)                                     # Hormone replacement therapy use
data$sha1 <- factor(data$sha1)                                   # Region - used for IECV

## Now, start converting these factors to dummies ##
## Set up list of parameters that need dummies ##
dummy_parameters <- c('radiotherapy', 'hrt', 'cancer_stage',
                      'cancer_grade', 'pr_status', 'er_status',  
                      'her2_status', 'cancer_route')

## The others will be hamdled as binary ##
## Generate the dummies ##
dummies <- dummyVars(~ smoke_cat + radiotherapy + mastectomy + 
                       other_surgery + hrt + cancer_stage + 
                       cancer_grade + pr_status2 + her2_status2 + er_status2 +
                       cancer_route, data = data)

## Package the dummy variables into its own dataset, ready to bind with the ##
## other, non-dummy parameters ##
dummied <- as.data.frame(predict(dummies, newdata=data))

## Form the ready-to-process dataset by binding the new dummies with the other 
#numeric parameters ##
data_for_model <- cbind(data[, -c(which(colnames(data) %in% dummy_parameters))], 
                        dummied)

## Create normalise function - continuous variables are to be normalised/scaled 
## to be between 0 and 1 ##

normalise <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

data_for_model$age_at_diagnosis  <- normalise(data_for_model$age)
data_for_model$bmi               <- normalise(data_for_model$bmi)

## Set up the predictors ('x cols') and outcome ('y cols') ##
## Rename variables (inc. new dummies) where relevant for interpretation ##
colnames(data_for_model)[colnames(data_for_model) == 'smoke_cat.0']     <- 'Non_smoker'
colnames(data_for_model)[colnames(data_for_model) == 'smoke_cat.1']     <- 'Ex_smoker'
colnames(data_for_model)[colnames(data_for_model) == 'smoke_cat.2']     <- 'Light_smoker'
colnames(data_for_model)[colnames(data_for_model) == 'smoke_cat.3']     <- 'Moderate_smoker'
colnames(data_for_model)[colnames(data_for_model) == 'smoke_cat.4']     <- 'Heavy_smoker'
colnames(data_for_model)[colnames(data_for_model) == 'radiotherapy.1']  <- 'Radiotherapy'
colnames(data_for_model)[colnames(data_for_model) == 'mastectomy.1']    <- 'Mastectomy'
colnames(data_for_model)[colnames(data_for_model) == 'other_surgery.1'] <- 'Other_surgery'
colnames(data_for_model)[colnames(data_for_model) == 'hrt.1']           <- 'HRT_use'
colnames(data_for_model)[colnames(data_for_model) == 'cancer_stage.1']  <- 'Stage1'
colnames(data_for_model)[colnames(data_for_model) == 'cancer_stage.2']  <- 'Stage2'
colnames(data_for_model)[colnames(data_for_model) == 'cancer_stage.3']  <- 'Stage3'
colnames(data_for_model)[colnames(data_for_model) == 'cancer_stage.4']  <- 'Stage4'
colnames(data_for_model)[colnames(data_for_model) == 'cancer_grade.1']  <- 'Well_differentiated'
colnames(data_for_model)[colnames(data_for_model) == 'cancer_grade.2']  <- 'Moderately_differentiated'
colnames(data_for_model)[colnames(data_for_model) == 'cancer_grade.3']  <- 'Poorly_differentiated'
colnames(data_for_model)[colnames(data_for_model) == 'pr_status2.3']    <- 'PR_positive'
colnames(data_for_model)[colnames(data_for_model) == 'her2_status2.3']  <- 'HER2_positive'
colnames(data_for_model)[colnames(data_for_model) == 'er_status2.3']    <- 'ER_positive' 
colnames(data_for_model)[colnames(data_for_model) == 'cancer_route.2']  <- 'Emergency_presentation'
colnames(data_for_model)[colnames(data_for_model) == 'cancer_route.3']  <- 'GP_referral'
colnames(data_for_model)[colnames(data_for_model) == 'cancer_route.4']  <- 'Inpatient_elective'
colnames(data_for_model)[colnames(data_for_model) == 'cancer_route.5']  <- 'Other_outpatient_pathway'
colnames(data_for_model)[colnames(data_for_model) == 'cancer_route.6']  <- 'Screening_detected'
colnames(data_for_model)[colnames(data_for_model) == 'cancer_route.7']  <- 'Two_week_wait'

## 'pseudo' are the jack-knife pseudo-values for the A-J CIF ##
x_cols <-  c('age_at_diagnosis', 'bmi', 'Non_smoker', 'Ex_smoker', 'Light_smoker', 
             'Moderate_smoker', 'Heavy_smoker', 'Radiotherapy', 'Mastectomy', 
             'Other_surgery', 'HRT_use', 'Stage1', 'Stage2', 'Stage3', 'Stage4', 'Well_differentiated', 
             'Moderately_differentiated', 'Poorly_differentiated', 'PR_positive', 
             'HER2_positive', 'ER_positive', 'Emergency_presentation', 'GP_referral', 'Inpatient_elective', 
             'Other_outpatient_pathway', 'Screening_detected', 'Two_week_wait')
y_cols <- c('pseudo')

## Clean up environment to avoid issues with memory ##
rm(dummied) 
rm(data) 


########################
## Dataset formatting ##
########################

## Change dataset to a matrix  ##
## x_train = predictor parameters ##
x_train <- as.matrix(data_for_model[, x_cols])

## Labels = the target for the neural net - the pseudovalues ##
label_train <- as.matrix(data_for_model[, y_cols])

## Number of input neurons (predictor parameters) 
dim(x_train) 


###############################################
## Define custom loss function for the model ##
###############################################

## Define root mean squared error loss function ##
## This is used to fit the neural network model ##

rmse <- function(y_true, y_pred){
  K <- backend()
  loss <- K$sqrt(K$mean((y_true - y_pred)^2))
  return(loss)
}

############################################################################
## FULL MODEL: Setting up Bayesian optimisation for hyperparameter tuning ##
############################################################################

## First, we fit neural network to entirety of data; Bayesian optimisation ## 
## used to identify optimal hyperparameters (with nested cross-val) ##

## Set up callbacks - use early stopping to stop training if models gets into ##
## a rut and there is no improvement. Combine with drop in learning rate if 
## hits plateau.
callbacks_list <- list(
         callback_reduce_lr_on_plateau(monitor = "loss", factor = 0.5, patience = 3),
         callback_early_stopping(monitor = "loss", min_delta = 0.0001, patience = 5)
         )


## seed for reproducibility ##
set.seed(2066)

## The Bayesian Optimisation package requires a 'scoring function' which ##
## it uses to assess the performance of the different configurations ##

## Herein, we specify that we want to tune the number of epochs,  ##
## number of units in each hidden layer, and number of ##
## hidden layers ##
## To estimate the performance of each model configuration, we use 5-fold ##
## cross-validation, so we nest a loop inside the scoring function so that ##
## the 'score' returned is a cross-validated estimate ##

scorefunction <- function(epochs, units, layers, learnrate) {
  
  set.seed(1066)
  
  k <- 5                                                                      ## 5-fold cross-validation 
  indices <- sample(1:nrow(x_train))                                          ## Used to randomly assign to folds 
  folds <- cut(1:length(indices), breaks = k, labels = FALSE)                 ## folds 
  
  cv_score <- c()                                                             ## empty list to store 'scores' over each CV iteration 
  
  for (i in 1:k) {                                                            ## Nested loop to perform CV
    
    cv_val_indices     <- which(folds==i)                                     ## validation set 1/5 folds 
    cv_val_data        <- x_train[cv_val_indices, ]                           ## Take from x_train 
    cv_val_labels      <- label_train[cv_val_indices]                         ## Take from labels 
    
    cv_train_data      <- x_train[-cv_val_indices, ]                          ## Train data = 4/5 folds
    cv_train_labels    <- label_train[-cv_val_indices]                        ## Train labels 4/5 
    
    cv_model <- keras_model_sequential()                                      ## Instantiate keras model 
    for(m in 1:layers){                                                       ## loop added to vary number of hidden layers
      cv_model %>% 
        layer_dense(units = units, activation = 'relu', 
                    input_shape=c(27))
    }
    cv_model %>% layer_dense(units = 1, activation = "linear") %>% 
      compile(
        optimizer = optimizer_adam(lr=learnrate),  
        loss = rmse,                                                          ## Use the above defined custom loss function
        metrics = "mse"                                                       ## Reported as model fits, but not a loss function 
      )
    ## Fit the model- batch size= 1024, weights applied for loss function ##
    cv_model %>% fit(cv_train_data, cv_train_labels, epochs=epochs, 
                     batch_size=1024, verbose=2, callbacks = callbacks_list, 
                     validation_data = list(cv_val_data, cv_val_labels))      ## Use heldout data to plot training dynamics   
    
    ## Generate predictions on held-out fold ##
    cv_predictions <- predict(cv_model, cv_val_data)
    
    ## BO wants to find the maximum, but lower RMSE = better ##
    ## -1 x RMSE: lower RMSE = higher value, so use this to score the config ##
    negative_rmse <- -1*sqrt((mean((cv_predictions-cv_val_labels)^2)))
    
    # Return score (or clipped score, if gradient exploded) to bayesian_neural 
    cv_score <- c(cv_score, negative_rmse)
    
    ## Clear out the model at the end of loop to avoid merging ##
    rm(cv_model) 
  }
  ## BO needs score to be returned to it ##
  return(list(Score = mean(cv_score)))
}


## Define hyperparameter search space - 'bounds' to search ##

bounds <- list(
  epochs = c(1L, 50L),                                                        ## Number of times model gets fed the data
  units = c(27L, 50L),                                                        ## Number of nodes in each layer 
  layers = c(1L, 5L),                                                         ## Between 1 and 5 hidden layers
  learnrate = c(0.001, 0.1)  
)

###############################################################################
## FULL MODEL: Running Bayesian optimisation to find optimal hyperparameters ##
###############################################################################

## Define Bayesian Optimisation function and run it ##
## Also keep track of time ##
start <- Sys.time() 

## Function takes in the scoring function we defined above, the bounds to ##
## search within, number of 'initialisations' to run first, then the number ## 
## of further iterations to try find the global optimum ##

bayesian_neural <- bayesOpt(
  FUN = scorefunction, 
  bounds = bounds, 
  initPoints = 25, 
  iters.n = 25, 
  iters.k = 1, 
  parallel = FALSE, 
  plotProgress = FALSE, 
  verbose = 2, 
  errorHandling = "continue", 
  acq = "ei"
)

end <- Sys.time() 
end-start 

## Summarise Bayesian Optimisation run ##
bayesian_neural$scoreSummary 

plot(bayesian_neural) 

bestpars <- getBestPars(bayesian_neural)
bestpars 

# Extract optimal hyperparameter config to fit final model on whole dataset # 
best_epochs <- bestpars[1]
best_units <- bestpars[2]
best_layers <- toString(bestpars[3])
best_learnrate <- as.numeric(bestpars[4]) 

callbacks_list <- list(
         callback_reduce_lr_on_plateau(monitor = "loss", factor = 0.5, patience = 3),
         callback_early_stopping(monitor = "loss", min_delta = 0.0001, patience = 5)
         )

## Use same format as above - define the 'final_model' and fit it ##
final_model <- keras_model_sequential() 
for(i in 1:best_layers){
  final_model %>% 
    layer_dense(units = best_units, activation = 'relu',
                input_shape=c(27))
}
final_model %>% layer_dense(units = 1, activation = "linear") %>% 
  compile(
    optimizer = optimizer_adam(lr=best_learnrate), 
    loss = rmse, 
    metrics = "mae"
  )

## Basic characteristics of final model, e.g. number of layers, parameters ##
summary(final_model) 

############################
# Fit final neural network #
############################

## Fit the model with said config to whole dataset, and save it ##
final_model %>% fit(x_train, label_train, epochs=best_epochs, 
                    batch_size=1024, verbose=2, callbacks=callbacks_list)
                    
## Save the model ##
setwd("/models/Endpoint_3/")    
save_model_hdf5(final_model, "ep3_neuralnetwork_final.h5")

###############################################################################################
###############################################################################################

## Clear out the environment to start fresh for evaluation ##
rm = ls() 


## MODEL EVALUATION STRATEGY ## 

#############################
## Load in packages needed ##
#############################
library(survival)
library(readr)
library(stringr)
library(caret)
library(haven)
library(ParBayesianOptimization) 
library(keras)
library(tensorflow)

########################################################
## Check and set working directory for importing data ##
########################################################
## Here, we need to separately import and format the Period 1 and Period 2 data ##
getwd() 
setwd("/final_datasets/ML/")

## Importing Period 1 sub-cohort (model fitting) separately from Period 2 sub-cohort (model evaluation) ##
data_period1 <- read_dta("OX129_endpoint3_stacked50_period1_pseudovalues.dta")
str(data_period1)

## Need to reformat categorical variables ##
## Categorical variables need to be converted into dummy variables ##
## First, reformat relevant parameters as factors, then convert to dummies #

data_period1$smoke_cat <- factor(data_period1$smoke_cat)                         # Smoking status 
data_period1$cancer_route <- factor(data_period1$cancer_route)                   # Route to diagnosis
data_period1$pr_status2 <- factor(data_period1$pr_status2)                       # PR +/-
data_period1$her2_status2 <- factor(data_period1$her2_status2)                   # HER2 +/-
data_period1$er_status2 <- factor(data_period1$er_status2)                       # ER +/- 
data_period1$radiotherapy <- factor(data_period1$radiotherapy)                   # Use of RTx in 1st year 
data_period1$mastectomy <- factor(data_period1$mastectomy)                       # Mastectomy in 1st year  
data_period1$other_surgery <- factor(data_period1$other_surgery)                 # Other surgery in 1st year 
data_period1$cancer_stage <- factor(data_period1$cancer_stage)                   # Stage (I-IV)
data_period1$cancer_grade <- factor(data_period1$cancer_grade)                   # Grade (differentiation)
data_period1$hrt <- factor(data_period1$hrt)                                     # Hormone replacement therapy use
data_period1$sha1 <- factor(data_period1$sha1)                                   # Region - used for IECV

## Now, start converting these factors to dummies ##
## Set up list of parameters that need dummies ##
dummy_parameters <- c('radiotherapy', 'hrt', 'cancer_stage',
                      'cancer_grade', 'pr_status', 'er_status',  
                      'her2_status', 'cancer_route')

## The others will be hamdled as binary ##
## Generate the dummies ##
dummies <- dummyVars(~ smoke_cat + radiotherapy + mastectomy + 
                       other_surgery + hrt + cancer_stage + 
                       cancer_grade + pr_status2 + her2_status2 + er_status2 +
                       cancer_route, data = data_period1)

# Package the dummy variables into its own dataset, ready to bind with the 
# other, non-dummy parameters #
dummied <- as.data.frame(predict(dummies, newdata=data_period1))

# Form the ready-to-process dataset by binding the new dummies with the other 
#numeric parameters #
data_period1 <- cbind(data_period1[, -c(which(colnames(data_period1) %in% dummy_parameters))], 
                        dummied)

# Use normalise function - continuous variables are to be normalised/scaled to 
# be between 0 and 1

normalise <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

data_period1$age_at_diagnosis  <- normalise(data_period1$age_at_diagnosis)
data_period1$bmi               <- normalise(data_period1$bmi)


## Set up the predictors ('x cols') and outcome ('y cols') ##
## Rename variables (inc. new dummies) where relevant for interpretation of 
## variable importance and other graphs later ##
colnames(data_period1)[colnames(data_period1) == 'smoke_cat.0']     <- 'Non_smoker'
colnames(data_period1)[colnames(data_period1) == 'smoke_cat.1']     <- 'Ex_smoker'
colnames(data_period1)[colnames(data_period1) == 'smoke_cat.2']     <- 'Light_smoker'
colnames(data_period1)[colnames(data_period1) == 'smoke_cat.3']     <- 'Moderate_smoker'
colnames(data_period1)[colnames(data_period1) == 'smoke_cat.4']     <- 'Heavy_smoker'
colnames(data_period1)[colnames(data_period1) == 'radiotherapy.1']  <- 'Radiotherapy'
colnames(data_period1)[colnames(data_period1) == 'mastectomy.1']    <- 'Mastectomy'
colnames(data_period1)[colnames(data_period1) == 'other_surgery.1'] <- 'Other_surgery'
colnames(data_period1)[colnames(data_period1) == 'hrt.1']           <- 'HRT_use'
colnames(data_period1)[colnames(data_period1) == 'cancer_stage.1']  <- 'Stage1'
colnames(data_period1)[colnames(data_period1) == 'cancer_stage.2']  <- 'Stage2'
colnames(data_period1)[colnames(data_period1) == 'cancer_stage.3']  <- 'Stage3'
colnames(data_period1)[colnames(data_period1) == 'cancer_stage.4']  <- 'Stage4'
colnames(data_period1)[colnames(data_period1) == 'cancer_grade.1']  <- 'Well_differentiated'
colnames(data_period1)[colnames(data_period1) == 'cancer_grade.2']  <- 'Moderately_differentiated'
colnames(data_period1)[colnames(data_period1) == 'cancer_grade.3']  <- 'Poorly_differentiated'
colnames(data_period1)[colnames(data_period1) == 'pr_status2.3']    <- 'PR_positive'
colnames(data_period1)[colnames(data_period1) == 'her2_status2.3']  <- 'HER2_positive'
colnames(data_period1)[colnames(data_period1) == 'er_status2.3']    <- 'ER_positive' 
colnames(data_period1)[colnames(data_period1) == 'cancer_route.2']  <- 'Emergency_presentation'
colnames(data_period1)[colnames(data_period1) == 'cancer_route.3']  <- 'GP_referral'
colnames(data_period1)[colnames(data_period1) == 'cancer_route.4']  <- 'Inpatient_elective'
colnames(data_period1)[colnames(data_period1) == 'cancer_route.5']  <- 'Other_outpatient_pathway'
colnames(data_period1)[colnames(data_period1) == 'cancer_route.6']  <- 'Screening_detected'
colnames(data_period1)[colnames(data_period1) == 'cancer_route.7']  <- 'Two_week_wait'

## 'pseudo' are the jack-knife pseudo-values for the Aalen-Johanssen CIF at 10 years ##
x_cols <-  c('age_at_diagnosis', 'bmi', 'Non_smoker', 'Ex_smoker', 'Light_smoker', 
             'Moderate_smoker', 'Heavy_smoker', 'Radiotherapy', 'Mastectomy', 
             'Other_surgery', 'HRT_use', 'Stage1', 'Stage2', 'Stage3', 'Stage4', 'Well_differentiated', 
             'Moderately_differentiated', 'Poorly_differentiated', 'PR_positive', 
             'HER2_positive', 'ER_positive', 'Emergency_presentation', 'GP_referral', 'Inpatient_elective', 
             'Other_outpatient_pathway', 'Screening_detected', 'Two_week_wait')
             
## Target for model fit is Period 1 pseudovalues ##
## Target for the evaluation are the Period 2 pseudovalues ##
y_cols <- c('period1_pseudo')

######################################
## NOW, REPEAT FOR PERIOD 2 DATASET ##
######################################

data_period2 <- read_dta("OX129_endpoint3_stacked50_period2_pseudovalues.dta")
str(data_period2)
## Categorical variables need to be converted into dummy variables ##
## First, reformat relevant parameters as factors, then convert to dummies #
data_period2$smoke_cat <- factor(data_period2$smoke_cat)                         # Smoking status 
data_period2$cancer_route <- factor(data_period2$cancer_route)                   # Route to diagnosis
data_period2$pr_status2 <- factor(data_period2$pr_status2)                       # PR +/-
data_period2$her2_status2 <- factor(data_period2$her2_status2)                   # HER2 +/-
data_period2$er_status2 <- factor(data_period2$er_status2)                       # ER +/- 
data_period2$radiotherapy <- factor(data_period2$radiotherapy)                   # Use of RTx in 1st year 
data_period2$mastectomy <- factor(data_period2$mastectomy)                       # Mastectomy in 1st year  
data_period2$other_surgery <- factor(data_period2$other_surgery)                 # Other surgery in 1st year 
data_period2$cancer_stage <- factor(data_period2$cancer_stage)                   # Stage (I-IV)
data_period2$cancer_grade <- factor(data_period2$cancer_grade)                   # Grade (differentiation)
data_period2$hrt <- factor(data_period2$hrt)                                     # Hormone replacement therapy use
data_period2$sha1 <- factor(data_period2$sha1)                                   # Region - used for IECV

# Generate the dummies #
dummies2 <- dummyVars(~ smoke_cat + radiotherapy + mastectomy + 
                       other_surgery + hrt + cancer_stage + 
                       cancer_grade + pr_status2 + her2_status2 + er_status2 +
                       cancer_route, data = data_period2)

# Package the dummy variables into its own dataset, ready to bind with the
# other, non-dummy parameters #
dummied2 <- as.data.frame(predict(dummies2, newdata=data_period2))

# Form the ready-to-process dataset by binding the new dummies with the other 
# numeric parameters #
data_period2 <- cbind(data_period2[, -c(which(colnames(data_period2) %in% 
                                                dummy_parameters))], dummied2)

data_period2$age_at_diagnosis  <- normalise(data_period2$age_at_diagnosis)
data_period2$bmi               <- normalise(data_period2$bmi)

colnames(data_period2)[colnames(data_period2) == 'smoke_cat.0']     <- 'Non_smoker'
colnames(data_period2)[colnames(data_period2) == 'smoke_cat.1']     <- 'Ex_smoker'
colnames(data_period2)[colnames(data_period2) == 'smoke_cat.2']     <- 'Light_smoker'
colnames(data_period2)[colnames(data_period2) == 'smoke_cat.3']     <- 'Moderate_smoker'
colnames(data_period2)[colnames(data_period2) == 'smoke_cat.4']     <- 'Heavy_smoker'
colnames(data_period2)[colnames(data_period2) == 'radiotherapy.1']  <- 'Radiotherapy'
colnames(data_period2)[colnames(data_period2) == 'mastectomy.1']    <- 'Mastectomy'
colnames(data_period2)[colnames(data_period2) == 'other_surgery.1'] <- 'Other_surgery'
colnames(data_period2)[colnames(data_period2) == 'hrt.1']           <- 'HRT_use'
colnames(data_period2)[colnames(data_period2) == 'cancer_stage.1']  <- 'Stage1'
colnames(data_period2)[colnames(data_period2) == 'cancer_stage.2']  <- 'Stage2'
colnames(data_period2)[colnames(data_period2) == 'cancer_stage.3']  <- 'Stage3'
colnames(data_period2)[colnames(data_period2) == 'cancer_stage.4']  <- 'Stage4'
colnames(data_period2)[colnames(data_period2) == 'cancer_grade.1']  <- 'Well_differentiated'
colnames(data_period2)[colnames(data_period2) == 'cancer_grade.2']  <- 'Moderately_differentiated'
colnames(data_period2)[colnames(data_period2) == 'cancer_grade.3']  <- 'Poorly_differentiated'
colnames(data_period2)[colnames(data_period2) == 'pr_status2.3']    <- 'PR_positive'
colnames(data_period2)[colnames(data_period2) == 'her2_status2.3']  <- 'HER2_positive'
colnames(data_period2)[colnames(data_period2) == 'er_status2.3']    <- 'ER_positive' 
colnames(data_period2)[colnames(data_period2) == 'cancer_route.2']  <- 'Emergency_presentation'
colnames(data_period2)[colnames(data_period2) == 'cancer_route.3']  <- 'GP_referral'
colnames(data_period2)[colnames(data_period2) == 'cancer_route.4']  <- 'Inpatient_elective'
colnames(data_period2)[colnames(data_period2) == 'cancer_route.5']  <- 'Other_outpatient_pathway'
colnames(data_period2)[colnames(data_period2) == 'cancer_route.6']  <- 'Screening_detected'
colnames(data_period2)[colnames(data_period2) == 'cancer_route.7']  <- 'Two_week_wait'

## Check dimensions of two sub-sets ##
dim(data_period1)
dim(data_period2) 

## Clean up memory by removing the dummied interim datasets 
rm(dummied) 
rm(dummied2) 

###############################################
## Define custom loss function for the model ##
###############################################

## Define root mean squared error loss function ##
## This is used to fit the neural network model ##

rmse <- function(y_true, y_pred){
  K <- backend()
  loss <- K$sqrt(K$mean((y_true - y_pred)^2))
  return(loss)
}

## Set up callbacks ##
callbacks_list <- list(
         callback_reduce_lr_on_plateau(monitor = "loss", factor = 0.5, patience = 3),
         callback_early_stopping(monitor = "loss", min_delta = 0.0001, patience = 5)
         )

##############################################################
## Define scoring function for nested cross-validation/IECV ##
##############################################################

## Need to use the period 1 data for tuning with 5-fold cross-validation ## 
## Use the period 2 data as the completely held-out test set(s) ## 
## Below, we set up a loop to iterate this over each region for IECV, but here, 
# first we set up the Bayesian Optimisation scoring function that will be embedded in this loop ##
set.seed(2066) 
  
scorefunction <- function(epochs, units, layers, learnrate){
  
  set.seed(2066)
  
  k <- 5 
  indices <- sample(1:nrow(data_fit))                               ## 'data_fit' will be generated from data_period1 subset 
  folds <- cut(1:length(indices), breaks = k, labels = FALSE)
  
  iecv_score <- c() 
  
  for (a in 1:k) {
    
    iecv_val_indices     <- which(folds==a)
    
    iecv_val_data        <- data_fit[iecv_val_indices, ]           ## validation is the heldout 1/5th
    iecv_val_labels      <- label_fit[iecv_val_indices]            ## for CV loop for hyperparam tuning 
    
    iecv_fit_data        <- data_fit[-iecv_val_indices, ]
    iecv_fit_labels      <- label_fit[-iecv_val_indices]
    
    bayes_model <- keras_model_sequential()                        ## Instantiate keras model 
    for(b in 1:layers){                                            ## loop added to vary number of hidden layers
      bayes_model %>% 
        layer_dense(units = units, activation = 'relu', 
                    input_shape=c(27))
    }
    bayes_model %>% layer_dense(units = 1, activation = "linear") %>% 
      compile(
        optimizer = optimizer_adam(lr=learnrate),  
        loss = rmse,                                                ## Use the above defined custom loss function
        metrics = "mse"                                             ## Reported as model fits, but not a loss function 
      )
    ## Fit the model- batch size= 1024, weights applied for loss function ##
    bayes_model %>% fit(iecv_fit_data, iecv_fit_labels, epochs=epochs, 
                     batch_size=1024, verbose=0, callbacks = callbacks_list, 
                     validation_data = list(iecv_val_data, iecv_val_labels)) 
    
    iecv_predictions <- predict(bayes_model, iecv_val_data)
    negative_rmse <- -1*sqrt(mean((iecv_predictions-iecv_val_labels)^2))
    iecv_score <- c(iecv_score, negative_rmse)
    
    rm(bayes_model) 
   }
  
  return(list(Score = mean(iecv_score)))                           ## Return the score (-ve RMSE obtained with each combination to Bayes Optim algorithm)
  
 }

#############################################################
## Define hyperparameter search space - 'bounds' to search ##
#############################################################

bounds <- list(
  epochs = c(1L, 20L),                                  ## Number of times model gets fed the data
  units = c(27L, 50L),                                  ## Number of nodes in each layer 
  layers = c(1L, 5L),                                   ## Between 1 and 5 hidden layers
  learnrate = c(0.001, 0.1)  
)

######################################################
## Define Bayesian Optimisation function and run it ##
######################################################

start <- Sys.time()                           ## Time taken is of interest 

iecv_predictions <- c() 

for(d in seq(1:10)) {                        ## 10 regions (sha1) - loop over each of them
  
  set.seed(1066)  
  
  data_1 <- data_period1[ -which (data_period1$sha1 == 
                                     levels(data_period1$sha1)[d]), ]          ## Fit data is all regions except region 'a'
  data_fit       <- as.matrix(data_1[, x_cols])                                ## Form this into matrix of predictors 
  label_fit      <- as.matrix(data_1[, y_cols])                                ## Form matrix of their pseudovalues
  
  data_2 <- data_period2[ which (data_period2$sha1 == 
                                    levels(data_period2$sha1)[d]), ]           ## Validation data is region 'a'
  data_val       <- as.matrix(data_2[, x_cols])                                ## Package matrix of predictors - no need for 'target' pseudovalues here
  
  bayesian_neural <- bayesOpt(                                                 ## Bayesian Optimisation command, will run scoring function above on this cross-val loop
  FUN = scorefunction, 
  bounds = bounds, 
  initPoints = 25, 
  iters.n = 25, 
  iters.k = 1, 
  parallel = FALSE, 
  plotProgress = FALSE, 
  verbose = 2, 
  errorHandling = "continue", 
  acq = "ei"
  ) 
 
  ## Summarise Bayesian Optimisation run ##
  bayesian_neural$scoreSummary 
  plot(bayesian_neural) 
  bestpars <- getBestPars(bayesian_neural)                                      ## Best parameter combination found in this iteration
  bestpars 

  # Extract optimal hyperparameter config to fit final model on whole 'fit' dataset # 
  best_epochs <- bestpars[1]
  best_units <- bestpars[2]
  best_layers <- toString(bestpars[3])
  best_learnrate <- as.numeric(bestpars[4]) 

  ## Use same format as above - define the 'final_model' and fit it ##
  iecv_model <- keras_model_sequential() 
  for(e in 1:best_layers){
    iecv_model %>% 
      layer_dense(units = best_units, activation = 'relu',
                input_shape=c(27))
  }
  iecv_model %>% layer_dense(units = 1, activation = "linear") %>% 
  compile(
    optimizer_adam(lr=best_learnrate), 
    loss = rmse, 
    metrics = "mse"
  )

  iecv_model %>% fit(data_fit, label_fit, epochs=best_epochs, 
                    batch_size=1024, verbose=0)                        ## Fit model to period 1 data from all regions except 'a'

predictions <- predict(iecv_model, data_val)                                         ## Test model on period 2 data for region 'a'
iecv_predictions[[d]] = predictions                                                  ## Store the predictions as loop iterates 

rm(iecv_model)                                                                       ## Clear out so that model can be refitted on next loop iteration without stacking networks...

}

end <- Sys.time() 
end-start                                                                            ## Took 2.08 days on my set-upwith GPU 


###########################################
# Combine predictions generated from IECV #
###########################################
nnet_iecv_predictions <-    c(iecv_predictions[[1]], iecv_predictions[[2]], 
                              iecv_predictions[[3]], iecv_predictions[[4]], 
                              iecv_predictions[[5]], iecv_predictions[[6]],
                              iecv_predictions[[7]], iecv_predictions[[8]], 
                              iecv_predictions[[9]], iecv_predictions[[10]])

summary(nnet_iecv_predictions)

## Combine with individual ID numbers so those can be linked up later ##

patid <- as.data.frame(data_period2$patid)
export_predictions <- cbind(patid, nnet_iecv_predictions)
head(export_predictions)

## Save these for use in Stata for evaluation ##

setwd("/estimates/ML_IECV_predictions/")
write.csv(export_predictions, "endpoint3_nnet_iecv_predictions.csv")


###############################################################################################
###############################################################################################
###############################################################################################

